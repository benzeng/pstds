**ä¸ªäººä¸“ç”¨è‚¡ç¥¨äº¤æ˜“å†³ç­–ç³»ç»Ÿ**

PSTDS â€” Personal Stock Trading Decision System

**è¯¦ç»†è®¾è®¡æ–‡æ¡£ï¼ˆDDDï¼‰v3.0**

å·¥ç¨‹è´¨é‡åŸºçº¿ + åŠŸèƒ½è¡¥å…¨ + ç»„åˆåˆ†ææ‰©å±• \| 2026å¹´3æœˆ \| ç‰ˆæœ¬ v3.0

# 1. é¡¹ç›®ç›®å½•ç»“æ„ï¼ˆv3.0 æ›´æ–°ï¼‰

v3.0 åœ¨ v2.0 ç›®å½•ç»“æ„åŸºç¡€ä¸Šæ–°å¢ pstds/portfolio/ æ¨¡å—ï¼Œè¡¥å…¨ memory/ ä¸‰ä¸ªæ–‡ä»¶ï¼Œæ–°å¢ pstds/llm/deepseek.py å’Œ dashscope.pyï¼Œæ–°å¢ pstds/backtest/report.pyï¼Œæ–°å¢ pstds/data/news_filter.pyï¼Œæ–°å¢ pstds/storage/models.pyã€‚

```python
pstds/ â† é¡¹ç›®æ ¹ç›®å½•  
â”œâ”€â”€ tradingagents/ â† åŸç‰ˆ TradingAgents æ ¸å¿ƒï¼ˆä¸ä¿®æ”¹ï¼‰  
â”‚  
â”œâ”€â”€ pstds/ â† æœ¬é¡¹ç›®æ‰©å±•ä»£ç   
â”‚ â”œâ”€â”€ temporal/ â† æ—¶é—´éš”ç¦»å±‚ï¼ˆâœ… v2.0 å·²å®ç°ï¼Œv3.0 ä¸å˜ï¼‰  
â”‚ â”‚ â”œâ”€â”€ context.py  
â”‚ â”‚ â”œâ”€â”€ guard.py  
â”‚ â”‚ â””â”€â”€ audit.py  
â”‚ â”‚  
â”‚ â”œâ”€â”€ data/ â† æ•°æ®æœåŠ¡å±‚  
â”‚ â”‚ â”œâ”€â”€ adapters/ â† å¸‚åœºæ•°æ®é€‚é…å™¨ï¼ˆâœ… v2.0 å·²å®ç°ï¼Œv3.0 ä¸å˜ï¼‰  
â”‚ â”‚ â”‚ â”œâ”€â”€ base.py  
â”‚ â”‚ â”‚ â”œâ”€â”€ yfinance_adapter.py  
â”‚ â”‚ â”‚ â”œâ”€â”€ akshare_adapter.py  
â”‚ â”‚ â”‚ â”œâ”€â”€ alphavantage_adapter.py  
â”‚ â”‚ â”‚ â””â”€â”€ local_csv_adapter.py  
â”‚ â”‚ â”œâ”€â”€ news_filter.py â† ğŸ†• v3.0 æ–°å¢ï¼šä¸‰çº§æ–°é—»è¿‡æ»¤å™¨  
â”‚ â”‚ â”œâ”€â”€ cache.py â† âœ… v2.0 å·²å®ç°ï¼ˆbug å·²ä¿®ï¼‰  
â”‚ â”‚ â”œâ”€â”€ router.py â† âœ… v2.0 å·²å®ç°ï¼ˆbug å·²ä¿®ï¼‰  
â”‚ â”‚ â”œâ”€â”€ quality_guard.py â† âœ… v2.0 å·²å®ç°ï¼ˆbug å·²ä¿®ï¼‰  
â”‚ â”‚ â”œâ”€â”€ models.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”‚ â””â”€â”€ fallback.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”‚  
â”‚ â”œâ”€â”€ agents/ â† Agent æ‰©å±•ï¼ˆâœ… v2.0 å·²å®ç°ï¼Œv3.0 ä¸å˜ï¼‰  
â”‚ â”‚ â”œâ”€â”€ extended_graph.py â† â™»ï¸ å¾®è°ƒï¼šNewsFilter è°ƒç”¨ç‚¹ã€retry æ”¹è¿›  
â”‚ â”‚ â”œâ”€â”€ debate_referee.py  
â”‚ â”‚ â”œâ”€â”€ result_saver.py  
â”‚ â”‚ â””â”€â”€ output_schemas.py  
â”‚ â”‚  
â”‚ â”œâ”€â”€ llm/ â† LLM é€‚é…å™¨æ‰©å±•  
â”‚ â”‚ â”œâ”€â”€ factory.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”‚ â”œâ”€â”€ deepseek.py â† ğŸ†• v3.0 æ–°å¢ï¼šDeepSeek é€‚é…å™¨  
â”‚ â”‚ â”œâ”€â”€ dashscope.py â† ğŸ†• v3.0 æ–°å¢ï¼šé˜¿é‡Œ Qwen é€‚é…å™¨  
â”‚ â”‚ â””â”€â”€ cost_estimator.py â† â™»ï¸ v3.0 æ›´æ–°ï¼šæ–°å¢ DeepSeek/Qwen è®¡è´¹è¡¨  
â”‚ â”‚  
â”‚ â”œâ”€â”€ portfolio/ â† ğŸ†• v3.0 å…¨æ–°æ¨¡å—ï¼šç»„åˆåˆ†æ  
â”‚ â”‚ â”œâ”€â”€ __init__.py  
â”‚ â”‚ â”œâ”€â”€ analyzer.py â† PortfolioAnalyzerï¼ˆç›¸å…³æ€§ã€VaRï¼‰  
â”‚ â”‚ â”œâ”€â”€ advisor.py â† PositionAdvisorï¼ˆä»“ä½å»ºè®®ï¼‰  
â”‚ â”‚ â””â”€â”€ models.py â† PortfolioAnalysisResultã€PositionAdvice ç­‰  
â”‚ â”‚  
â”‚ â”œâ”€â”€ backtest/ â† å›æµ‹å¼•æ“  
â”‚ â”‚ â”œâ”€â”€ runner.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”‚ â”œâ”€â”€ calendar.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”‚ â”œâ”€â”€ portfolio.py â† âœ… v2.0 å·²å®ç°ï¼ˆbug å·²ä¿®ï¼‰  
â”‚ â”‚ â”œâ”€â”€ executor.py â† âœ… v2.0 å·²å®ç°ï¼ˆbug å·²ä¿®ï¼‰  
â”‚ â”‚ â”œâ”€â”€ performance.py â† âœ… v2.0 å·²å®ç°ï¼ˆbug å·²ä¿®ï¼‰  
â”‚ â”‚ â””â”€â”€ report.py â† ğŸ†• v3.0 æ–°å¢ï¼šBacktestReportGenerator  
â”‚ â”‚  
â”‚ â”œâ”€â”€ memory/ â† è®°å¿†ç³»ç»Ÿ  
â”‚ â”‚ â”œâ”€â”€ short_term.py â† ğŸ†• v3.0 æ–°å¢ï¼šçŸ­æœŸå·¥ä½œè®°å¿†  
â”‚ â”‚ â”œâ”€â”€ episodic.py â† â™»ï¸ v3.0 è¡¥å…¨ï¼šéª¨æ¶ â†’ å®Œæ•´å®ç°  
â”‚ â”‚ â”œâ”€â”€ pattern.py â† ğŸ†• v3.0 æ–°å¢ï¼šé•¿æœŸæ¨¡å¼è®°å¿†  
â”‚ â”‚ â””â”€â”€ reflection.py â† ğŸ†• v3.0 æ–°å¢ï¼šåäº‹å®è®°å¿†ä¸æç‚¼  
â”‚ â”‚  
â”‚ â”œâ”€â”€ scheduler/ â† ä»»åŠ¡è°ƒåº¦ï¼ˆâœ… v2.0 å·²å®ç°ï¼‰  
â”‚ â”‚ â”œâ”€â”€ scheduler.py â† â™»ï¸ v3.0 æ–°å¢åäº‹å®è®°å¿†å‘¨ä»»åŠ¡  
â”‚ â”‚ â””â”€â”€ task_queue.py  
â”‚ â”‚  
â”‚ â”œâ”€â”€ storage/ â† æŒä¹…åŒ–å±‚  
â”‚ â”‚ â”œâ”€â”€ mongo_store.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”‚ â”œâ”€â”€ watchlist_store.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”‚ â””â”€â”€ models.py â† ğŸ†• v3.0 æ–°å¢ï¼šMongoDB æ–‡æ¡£æ¨¡å‹å®šä¹‰  
â”‚ â”‚  
â”‚ â”œâ”€â”€ export/ â† æŠ¥å‘Šå¯¼å‡ºï¼ˆâœ… v2.0 å·²å®ç°ï¼‰  
â”‚ â”‚ â”œâ”€â”€ pdf_exporter.py â† â™»ï¸ v3.0 æ”¯æŒç»„åˆ/å›æµ‹æŠ¥å‘Š  
â”‚ â”‚ â”œâ”€â”€ docx_exporter.py â† â™»ï¸ v3.0 æ”¯æŒç»„åˆ/å›æµ‹æŠ¥å‘Š  
â”‚ â”‚ â””â”€â”€ md_exporter.py  
â”‚ â”‚  
â”‚ â”œâ”€â”€ notify/ â† é€šçŸ¥æ¨¡å—ï¼ˆâœ… v2.0 å·²å®ç°ï¼Œv3.0 ä¸å˜ï¼‰  
â”‚ â”‚ â”œâ”€â”€ desktop.py  
â”‚ â”‚ â””â”€â”€ email_notify.py  
â”‚ â”‚  
â”‚ â””â”€â”€ config.py â† âœ… v2.0 å·²å®ç°ï¼ˆS1 bug å·²ä¿®ï¼‰  
â”‚  
â”œâ”€â”€ web/ â† Streamlit Web App  
â”‚ â”œâ”€â”€ app.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”œâ”€â”€ pages/  
â”‚ â”‚ â”œâ”€â”€ 01_analysis.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”‚ â”œâ”€â”€ 02_watchlist.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”‚ â”œâ”€â”€ 03_history.py â† â™»ï¸ v3.0 æ–°å¢å†³ç­–å‡†ç¡®ç‡è¶‹åŠ¿å›¾  
â”‚ â”‚ â”œâ”€â”€ 04_backtest.py â† â™»ï¸ v3.0 å½’å› åˆ†æã€å†å²æŠ¥å‘Šæ£€ç´¢  
â”‚ â”‚ â”œâ”€â”€ 05_cost.py â† âœ… v2.0 å·²å®ç°  
â”‚ â”‚ â”œâ”€â”€ 06_portfolio.py â† â™»ï¸ v3.0 ä¸ PortfolioAnalyzer è”åŠ¨  
â”‚ â”‚ â”œâ”€â”€ 07_settings.py â† â™»ï¸ v3.0 æ–°å¢ DeepSeek/Qwen é…ç½®  
â”‚ â”‚ â””â”€â”€ 08_portfolio_analysis.py â† ğŸ†• v3.0 å…¨æ–°ï¼šç»„åˆåˆ†æé¡µé¢  
â”‚ â””â”€â”€ components/  
â”‚ â”œâ”€â”€ chart.py â† â™»ï¸ v3.0 å…¨å±æ¨¡å¼ã€æ—¶é—´å‘¨æœŸã€æ·±è‰²ä¸»é¢˜  
â”‚ â””â”€â”€ report_card.py â† âœ… v2.0 å·²å®ç°  
â”‚  
â”œâ”€â”€ tests/  
â”‚ â”œâ”€â”€ unit/  
â”‚ â”‚ â”œâ”€â”€ test_temporal_guard.py â† âœ… TG-001~TG-012 å…¨éƒ¨é€šè¿‡  
â”‚ â”‚ â”œâ”€â”€ test_output_schemas.py â† âœ… PM-001~PM-008 å…¨éƒ¨é€šè¿‡  
â”‚ â”‚ â”œâ”€â”€ test_market_router.py â† âœ… RT-001~RT-008 å…¨éƒ¨é€šè¿‡  
â”‚ â”‚ â”œâ”€â”€ test_news_filter.py â† ğŸ†• v3.0 æ–°å¢ï¼ˆNF ç³»åˆ—æµ‹è¯•ç”¨ä¾‹ï¼‰  
â”‚ â”‚ â”œâ”€â”€ test_portfolio_analyzer.py â† ğŸ†• v3.0 æ–°å¢ï¼ˆPA ç³»åˆ—æµ‹è¯•ç”¨ä¾‹ï¼‰  
â”‚ â”‚ â””â”€â”€ test_memory_system.py â† ğŸ†• v3.0 æ–°å¢ï¼ˆMEM ç³»åˆ—æµ‹è¯•ç”¨ä¾‹ï¼‰  
â”‚ â”œâ”€â”€ adapters/ â† âœ… v2.0 å·²å®ç°  
â”‚ â””â”€â”€ integration/ â† âœ… v2.0 å·²å®ç°ï¼Œv3.0 è¡¥å……ç»„åˆåˆ†æé›†æˆæµ‹è¯•  
â”‚  
â”œâ”€â”€ config/  
â”‚ â”œâ”€â”€ default.yaml â† â™»ï¸ v3.0 æ–°å¢ portfolio/memory/llm é…ç½®é¡¹  
â”‚ â””â”€â”€ user.yaml â† gitignoreï¼ˆå·²ä¿®å¤ S1 å®‰å…¨é—®é¢˜ï¼‰  
â”‚  
â”œâ”€â”€ data/ â† è¿è¡Œæ—¶æ•°æ®ï¼ˆgitignoreï¼‰  
â”œâ”€â”€ docker-compose.yml â† â™»ï¸ v3.0 æ–°å¢ç¯å¢ƒå˜é‡  
â”œâ”€â”€ Dockerfile  
â”œâ”€â”€ requirements.txt â† â™»ï¸ v3.0 æ–°å¢ä¾èµ–  
â””â”€â”€ start.py
```

# 2. æ–°å¢/è¡¥å…¨æ¨¡å—è¯¦ç»†è®¾è®¡

## 2.1 pstds/data/news_filter.pyï¼ˆv3.0 æ–°å¢ï¼‰

```python
# pstds/data/news_filter.py  
  
from dataclasses import dataclass  
from typing import List, Optional  
from datetime import datetime  
import time  
from sklearn.feature_extraction.text import TfidfVectorizer  
from sklearn.metrics.pairwise import cosine_similarity  
import numpy as np  
from pstds.data.models import NewsItem  
from pstds.temporal.context import TemporalContext  
from pstds.temporal.guard import TemporalGuard  
  
@dataclass  
class NewsFilterResult:  
passed: List[NewsItem]  
dropped_future: int  
dropped_irrelevant: int  
dropped_duplicate: int  
filter_duration_ms: float  
  
class NewsFilter:  
def __init__(  
self,  
relevance_threshold: float = 0.6, # ç¬¬äºŒçº§ç›¸å…³æ€§é˜ˆå€¼  
dedup_threshold: float = 0.85, # ç¬¬ä¸‰çº§å»é‡é˜ˆå€¼  
):  
self.relevance_threshold = relevance_threshold  
self.dedup_threshold = dedup_threshold  
  
def filter(  
self,  
news_list: List[NewsItem],  
symbol: str,  
ctx: TemporalContext,  
keywords: Optional[List[str]] = None,  
) -> NewsFilterResult:  
t0 = time.monotonic()  
  
# ç¬¬ä¸€çº§ï¼šæ—¶é—´éš”ç¦»ï¼ˆå§”æ‰˜ TemporalGuardï¼‰  
compliant = TemporalGuard.filter_news(news_list, ctx)  
dropped_future = len(news_list) - len(compliant)  
  
# ç¬¬äºŒçº§ï¼šè¯­ä¹‰ç›¸å…³æ€§è¯„åˆ†  
kw = keywords or [symbol]  
relevant = [n for n in compliant if self._score_relevance(n, kw) >= self.relevance_threshold]  
dropped_irrelevant = len(compliant) - len(relevant)  
  
# ç¬¬ä¸‰çº§ï¼šè¯­ä¹‰å»é‡ï¼ˆä¿ç•™æ—¶é—´æœ€æ—©çš„ï¼‰  
deduped = self._deduplicate(relevant)  
dropped_duplicate = len(relevant) - len(deduped)  
  
return NewsFilterResult(  
passed=deduped,  
dropped_future=dropped_future,  
dropped_irrelevant=dropped_irrelevant,  
dropped_duplicate=dropped_duplicate,  
filter_duration_ms=(time.monotonic() - t0) * 1000,  
)  
  
def _score_relevance(self, news: NewsItem, keywords: List[str]) -> float:  
"""TF-IDF ä½™å¼¦ç›¸ä¼¼åº¦ï¼›è‹¥ ChromaDB å‘é‡å¯ç”¨åˆ™ä½¿ç”¨å¥å‘é‡"""  
text = f"{news.title} {news.content}"  
query = " ".join(keywords)  
try:  
vec = TfidfVectorizer().fit_transform([text, query])  
return float(cosine_similarity(vec[0], vec[1])[0][0])  
except Exception:  
return 1.0 # æ— æ³•è®¡ç®—æ—¶é»˜è®¤é€šè¿‡  
  
def _deduplicate(self, news_list: List[NewsItem]) -> List[NewsItem]:  
"""ä¸¤ä¸¤ç›¸ä¼¼åº¦ > dedup_threshold æ—¶ä¿ç•™æœ€æ—©ä¸€æ¡"""  
if len(news_list) <= 1:  
return news_list  
texts = [f"{n.title} {n.content}" for n in news_list]  
try:  
tfidf = TfidfVectorizer().fit_transform(texts)  
sim_matrix = cosine_similarity(tfidf)  
except Exception:  
return news_list  
kept_indices = []  
removed = set()  
sorted_by_time = sorted(range(len(news_list)),  
key=lambda i: news_list[i].published_at)  
for i in sorted_by_time:  
if i in removed:  
continue  
kept_indices.append(i)  
for j in sorted_by_time:  
if j != i and j not in removed and sim_matrix[i][j] > self.dedup_threshold:  
removed.add(j)  
return [news_list[i] for i in sorted(kept_indices,  
key=lambda i: news_list[i].published_at)]
```

## 2.2 pstds/portfolio/models.pyï¼ˆv3.0 æ–°å¢ï¼‰

```python
# pstds/portfolio/models.py  
  
from dataclasses import dataclass, field  
from typing import List, Dict, Tuple, Optional  
from datetime import date  
import pandas as pd  
  
@dataclass  
class PortfolioAnalysisResult:  
symbols: List[str]  
analysis_date: date  
correlation_matrix: pd.DataFrame # shape: nÃ—nï¼Œindex/columns ä¸º symbol  
high_correlation_pairs: List[Tuple[str,str,float]] # (sym1, sym2, corr) corr > 0.7  
sector_weights: Dict[str, float] # {sector_name: weight}  
sector_concentration_warning: bool # ä»»ä¸€è¡Œä¸šæƒé‡ > 40%  
portfolio_var_95: Optional[float] # å†å²æ¨¡æ‹Ÿ VaRï¼ˆä»…æœ‰æŒä»“æƒé‡æ—¶è®¡ç®—ï¼‰  
homogeneity_warning: bool # å‡å€¼ç›¸å…³æ€§ > 0.6  
data_quality_score: float # 0-100  
analysis_id: str # UUID  
  
@dataclass  
class PositionAdvice:  
weights: Dict[str, float] # {symbol: recommended_weight}ï¼Œæ€»å’Œ = 1.0  
rationale: Dict[str, str] # {symbol: å»ºè®®åŸå› è¯´æ˜}  
risk_warnings: List[str] # ç»„åˆçº§é£é™©è­¦å‘Šåˆ—è¡¨  
optimization_method: str # 'equal_weight'|'confidence_weighted'|'max_sharpe'  
constraint_violations: List[str] # çº¦æŸè¿åè¯´æ˜ï¼ˆè‹¥æœ‰ï¼‰
```

## 2.3 pstds/portfolio/analyzer.pyï¼ˆv3.0 æ–°å¢ï¼‰

```python
# pstds/portfolio/analyzer.py  
  
import uuid  
import numpy as np  
import pandas as pd  
from typing import List, Optional, Dict  
from datetime import date  
from pstds.portfolio.models import PortfolioAnalysisResult  
from pstds.temporal.context import TemporalContext  
from pstds.data.router import DataRouter  
  
class PortfolioAnalyzer:  
def __init__(self, data_router: DataRouter):  
self._router = data_router  
  
def analyze(  
self,  
symbols: List[str],  
ctx: TemporalContext, # å¿…å¡«ï¼ŒTemporalGuard æ ¡éªŒ  
positions: Optional[Dict[str, float]] = None, # {symbol: weight}  
lookback_days: int = 60,  
) -> PortfolioAnalysisResult:  
assert len(symbols) <= 20, "æœ€å¤šæ”¯æŒ 20 åªè‚¡ç¥¨"  
  
# 1. æ‰¹é‡è·å– OHLCVï¼ˆå…±äº«åŒä¸€ TemporalContextï¼‰  
ohlcv_batch = {}  
quality_scores = []  
for sym in symbols:  
df = self._router.get_ohlcv(sym, lookback_days=lookback_days, ctx=ctx)  
if df is not None and len(df) >= 10:  
ohlcv_batch[sym] = df  
quality_scores.append(min(len(df) / lookback_days * 100, 100))  
  
if len(ohlcv_batch) < 2:  
# æ•°æ®ä¸è¶³ï¼Œé€€åŒ–ä¸ºç©ºç»“æœ  
return self._empty_result(symbols, ctx.analysis_date)  
  
# 2. è®¡ç®—æ—¥æ”¶ç›Šç‡çŸ©é˜µ  
returns = pd.DataFrame({  
sym: df['close'].pct_change().dropna()  
for sym, df in ohlcv_batch.items()  
}).dropna()  
  
# 3. ç›¸å…³æ€§çŸ©é˜µ  
corr_matrix = returns.corr()  
high_pairs = [  
(s1, s2, float(corr_matrix.loc[s1, s2]))  
for i, s1 in enumerate(corr_matrix.columns)  
for j, s2 in enumerate(corr_matrix.columns)  
if i < j and abs(corr_matrix.loc[s1, s2]) > 0.7  
]  
  
# 4. ç»„åˆ VaRï¼ˆå†å²æ¨¡æ‹Ÿæ³•ï¼Œ95% ç½®ä¿¡åº¦ï¼‰  
var_95 = None  
if positions:  
w = np.array([positions.get(s, 0) for s in returns.columns])  
portfolio_returns = returns.values @ w  
var_95 = float(np.percentile(portfolio_returns, 5))  
  
# 5. è¡Œä¸šé›†ä¸­åº¦ï¼ˆä»åŸºæœ¬é¢æ•°æ®è·å– sectorï¼‰  
sector_weights = self._calc_sector_weights(symbols, positions, ctx)  
sector_warning = any(v > 0.4 for v in sector_weights.values())  
  
# 6. åŒè´¨åŒ–é¢„è­¦  
mean_corr = float(corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)].mean())  
homogeneity_warning = mean_corr > 0.6  
  
return PortfolioAnalysisResult(  
symbols=list(ohlcv_batch.keys()),  
analysis_date=ctx.analysis_date,  
correlation_matrix=corr_matrix,  
high_correlation_pairs=high_pairs,  
sector_weights=sector_weights,  
sector_concentration_warning=sector_warning,  
portfolio_var_95=var_95,  
homogeneity_warning=homogeneity_warning,  
data_quality_score=float(np.mean(quality_scores)) if quality_scores else 0.0,  
analysis_id=str(uuid.uuid4()),  
)  
  
def _calc_sector_weights(self, symbols, positions, ctx) -> Dict[str, float]:  
"""è·å–å„è‚¡è¡Œä¸šï¼Œè®¡ç®—æŒä»“æƒé‡åˆ†å¸ƒ"""  
sector_map = {}  
for sym in symbols:  
try:  
fund = self._router.get_fundamentals(sym, ctx.analysis_date, ctx)  
sector_map[sym] = fund.get('sector', 'Unknown') if fund else 'Unknown'  
except Exception:  
sector_map[sym] = 'Unknown'  
if not positions:  
equal_w = 1.0 / len(symbols)  
positions = {s: equal_w for s in symbols}  
result: Dict[str, float] = {}  
for sym, sector in sector_map.items():  
result[sector] = result.get(sector, 0) + positions.get(sym, 0)  
return result  
  
def _empty_result(self, symbols, analysis_date) -> PortfolioAnalysisResult:  
return PortfolioAnalysisResult(  
symbols=symbols, analysis_date=analysis_date,  
correlation_matrix=pd.DataFrame(), high_correlation_pairs=[],  
sector_weights={}, sector_concentration_warning=False,  
portfolio_var_95=None, homogeneity_warning=False,  
data_quality_score=0.0, analysis_id=str(uuid.uuid4()),  
)
```

## 2.4 pstds/portfolio/advisor.pyï¼ˆv3.0 æ–°å¢ï¼‰

```python
# pstds/portfolio/advisor.py  
  
from typing import List, Optional, Dict, Literal  
import numpy as np  
from pstds.portfolio.models import PortfolioAnalysisResult, PositionAdvice  
from pstds.agents.output_schemas import TradeDecision  
  
# å•åªè‚¡ç¥¨æœ€å¤§ä»“ä½  
MAX_SINGLE_POSITION = 0.30  
# é«˜ç›¸å…³å¯¹ï¼ˆ>0.7ï¼‰æ€»ä»“ä½ä¸Šé™  
MAX_CORR_PAIR_POSITION = 0.50  
  
class PositionAdvisor:  
def advise(  
self,  
decisions: List[TradeDecision],  
analysis: PortfolioAnalysisResult,  
risk_profile: Literal['conservative', 'balanced', 'aggressive'],  
current_positions: Optional[Dict[str, float]] = None,  
) -> PositionAdvice:  
# è¿‡æ»¤ BUY/STRONG_BUY çš„å†³ç­–  
buy_decisions = [d for d in decisions if d.action in ('BUY', 'STRONG_BUY', 'HOLD')]  
if not buy_decisions:  
return PositionAdvice(  
weights={d.symbol: 0.0 for d in decisions},  
rationale={d.symbol: 'æ—  BUY/HOLD ä¿¡å·' for d in decisions},  
risk_warnings=['æ‰€æœ‰è‚¡ç¥¨å‡ä¸º SELL/INSUFFICIENT_DATAï¼Œå»ºè®®ç©ºä»“'],  
optimization_method='none',  
constraint_violations=[],  
)  
  
# æŒ‰ç½®ä¿¡åº¦åŠ æƒï¼ˆåŸºç¡€ï¼‰  
method = 'confidence_weighted'  
symbols = [d.symbol for d in buy_decisions]  
confs = np.array([d.confidence for d in buy_decisions])  
raw_weights = confs / confs.sum()  
  
# åº”ç”¨ä¿å®ˆ/æ¿€è¿›è°ƒæ•´  
if risk_profile == 'conservative':  
raw_weights = raw_weights * 0.7 # æœ€å¤šæŒä»“ 70%ï¼Œç•™ 30% ç°é‡‘  
elif risk_profile == 'aggressive':  
pass # å…¨ä»“  
  
# åº”ç”¨å•åªä¸Šé™çº¦æŸ  
weights = np.minimum(raw_weights, MAX_SINGLE_POSITION)  
if weights.sum() > 0:  
weights = weights / weights.sum() * min(raw_weights.sum(), 1.0)  
  
weights_dict = {sym: float(w) for sym, w in zip(symbols, weights)}  
  
# æ£€æŸ¥é«˜ç›¸å…³å¯¹çº¦æŸ  
violations = []  
for s1, s2, corr in analysis.high_correlation_pairs:  
if s1 in weights_dict and s2 in weights_dict:  
pair_weight = weights_dict[s1] + weights_dict[s2]  
if pair_weight > MAX_CORR_PAIR_POSITION:  
violation = f"{s1}+{s2} ç›¸å…³æ€§ {corr:.2f}ï¼Œåˆè®¡ä»“ä½ {pair_weight:.1%} è¶…è¿‡ä¸Šé™ {MAX_CORR_PAIR_POSITION:.0%}"  
violations.append(violation)  
# ç­‰æ¯”ç¼©å‡  
factor = MAX_CORR_PAIR_POSITION / pair_weight  
weights_dict[s1] *= factor  
weights_dict[s2] *= factor  
  
# è¡¥å…¨ SELL/INSUFFICIENT_DATA çš„è‚¡ç¥¨ï¼ˆæƒé‡ä¸º 0ï¼‰  
for d in decisions:  
if d.symbol not in weights_dict:  
weights_dict[d.symbol] = 0.0  
  
rationale = {  
d.symbol: (f"ç½®ä¿¡åº¦ {d.confidence:.0%}ï¼Œ{d.action}ï¼›å»ºè®®ä»“ä½ {weights_dict[d.symbol]:.1%}"  
if d.action in ('BUY','STRONG_BUY','HOLD')  
else f"{d.action}ï¼Œä¸å»ºè®®æŒä»“")  
for d in decisions  
}  
  
risk_warnings = []  
if analysis.homogeneity_warning:  
risk_warnings.append("âš ï¸ ç»„åˆé«˜åº¦åŒè´¨åŒ–ï¼ˆå¹³å‡ç›¸å…³æ€§ > 0.6ï¼‰ï¼Œæ³¢åŠ¨é£é™©é›†ä¸­")  
if analysis.sector_concentration_warning:  
risk_warnings.append("âš ï¸ è¡Œä¸šé›†ä¸­åº¦è¿‡é«˜ï¼ˆæŸè¡Œä¸šæƒé‡ > 40%ï¼‰")  
  
return PositionAdvice(  
weights=weights_dict,  
rationale=rationale,  
risk_warnings=risk_warnings,  
optimization_method=method,  
constraint_violations=violations,  
)
```

## 2.5 pstds/memory/short_term.pyï¼ˆv3.0 æ–°å¢ï¼‰

```python
# pstds/memory/short_term.py  
  
import json  
import uuid  
from pathlib import Path  
from typing import Optional  
from datetime import datetime  
  
SNAPSHOT_DIR = Path("./data/snapshots")  
MAX_SNAPSHOTS = 5  
  
class ShortTermMemory:  
"""çŸ­æœŸå·¥ä½œè®°å¿†ï¼šGraphState å¿«ç…§åºåˆ—åŒ–ï¼Œæ”¯æŒä¼šè¯æ¢å¤"""  
  
def __init__(self, snapshot_dir: Path = SNAPSHOT_DIR):  
self.snapshot_dir = snapshot_dir  
self.snapshot_dir.mkdir(parents=True, exist_ok=True)  
  
def save_snapshot(self, state: dict, analysis_id: str) -> str:  
"""åºåˆ—åŒ– GraphState å¿«ç…§åˆ°æ–‡ä»¶ï¼Œè¿”å›å¿«ç…§ ID"""  
snapshot_id = f"{analysis_id}_{datetime.utcnow().strftime('%Y%m%dT%H%M%S')}"  
path = self.snapshot_dir / f"{snapshot_id}.json"  
# è¿‡æ»¤ä¸å¯åºåˆ—åŒ–çš„å­—æ®µï¼ˆå¦‚ DataFrameï¼‰  
serializable = {k: v for k, v in state.items()  
if isinstance(v, (str, int, float, bool, list, dict, type(None)))}  
path.write_text(json.dumps(serializable, default=str, ensure_ascii=False))  
self._cleanup_old_snapshots()  
return snapshot_id  
  
def restore_snapshot(self, snapshot_id: str) -> Optional[dict]:  
"""ä»æ–‡ä»¶æ¢å¤ GraphState å¿«ç…§"""  
path = self.snapshot_dir / f"{snapshot_id}.json"  
if not path.exists():  
return None  
return json.loads(path.read_text())  
  
def _cleanup_old_snapshots(self):  
"""ä¿ç•™æœ€è¿‘ MAX_SNAPSHOTS ä¸ªå¿«ç…§"""  
files = sorted(self.snapshot_dir.glob("*.json"), key=lambda f: f.stat().st_mtime)  
for old in files[:-MAX_SNAPSHOTS]:  
old.unlink(missing_ok=True)
```

## 2.6 pstds/memory/pattern.pyï¼ˆv3.0 æ–°å¢ï¼‰

```python
# pstds/memory/pattern.py  
  
from dataclasses import dataclass  
from typing import List, Optional  
from datetime import datetime  
  
@dataclass  
class MemoryPattern:  
pattern_key: str # å¦‚ "AAPL_high_volatility_bearish_signal"  
description: str # å¯è¯»æè¿°  
symbol: Optional[str] # ç‰¹å®šè‚¡ç¥¨ï¼ˆNone è¡¨ç¤ºé€šç”¨è§„å¾‹ï¼‰  
market_condition: str # 'trending_up'|'trending_down'|'ranging'|'high_volatility'  
win_rate: float # å†å²èƒœç‡ï¼ˆæ–¹å‘é¢„æµ‹å‡†ç¡®ç‡ï¼‰  
sample_count: int # æ ·æœ¬æ•°  
evidence_ids: List[str] # æ”¯æ’‘æ­¤è§„å¾‹çš„ reflection_record IDs  
last_updated: datetime  
is_active: bool # èƒœç‡ä½äº 55% åæ ‡è®°ä¸ºéæ´»è·ƒ  
  
class PatternMemory:  
"""é•¿æœŸæ¨¡å¼è®°å¿†ï¼Œä» MongoDB memory_patterns é›†åˆè¯»å†™"""  
  
def __init__(self, mongo_store):  
self._store = mongo_store  
  
def add_or_update_pattern(self, pattern: MemoryPattern) -> None:  
"""æ–°å¢æˆ–æ›´æ–°ï¼ˆupsertï¼Œä»¥ pattern_key ä¸ºå”¯ä¸€é”®ï¼‰"""  
self._store.upsert_pattern(pattern)  
  
def get_patterns(self, symbol: Optional[str] = None,  
min_win_rate: float = 0.6,  
min_samples: int = 10) -> List[MemoryPattern]:  
"""è·å–æ´»è·ƒè§„å¾‹ï¼ŒæŒ‰ win_rate é™åº"""  
return self._store.query_patterns(symbol=symbol, min_win_rate=min_win_rate,  
min_samples=min_samples, is_active=True)
```

## 2.7 pstds/memory/reflection.pyï¼ˆv3.0 æ–°å¢ï¼‰

```python
# pstds/memory/reflection.py  
  
from dataclasses import dataclass  
from typing import List, Optional  
from datetime import date, timedelta  
  
@dataclass  
class ReflectionRecord:  
analysis_id: str  
symbol: str  
analysis_date: date  
predicted_action: str # BUY/SELL/HOLD ç­‰  
predicted_confidence: float  
actual_return_next_day: Optional[float] # T+1 å®é™…æ¶¨è·Œå¹…  
is_direction_correct: Optional[bool] # é¢„æµ‹æ–¹å‘ä¸å®é™…æ¶¨è·Œæ˜¯å¦ä¸€è‡´  
created_at: str  
  
@dataclass  
class MonthlyAccuracy:  
year_month: str # '2024-03'  
total: int  
correct: int  
accuracy: float # correct / total  
  
@dataclass  
class RefinementReport:  
patterns_added: int  
patterns_updated: int  
samples_processed: int  
  
class ReflectionEngine:  
def __init__(self, mongo_store, data_router, pattern_memory):  
self._store = mongo_store  
self._router = data_router  
self._patterns = pattern_memory  
  
def record_outcome(self, analysis_id: str,  
symbol: str,  
analysis_date: date,  
predicted_action: str,  
predicted_confidence: float) -> None:  
"""T+1 è‡ªåŠ¨è·å–å®é™…ä»·æ ¼å˜åŒ–ï¼Œå†™å…¥ reflection_records"""  
next_day = self._get_next_trading_day(analysis_date, symbol)  
actual_return = self._fetch_actual_return(symbol, analysis_date, next_day)  
is_correct = self._evaluate_direction(predicted_action, actual_return)  
record = ReflectionRecord(  
analysis_id=analysis_id, symbol=symbol,  
analysis_date=analysis_date, predicted_action=predicted_action,  
predicted_confidence=predicted_confidence,  
actual_return_next_day=actual_return,  
is_direction_correct=is_correct,  
created_at=str(date.today()),  
)  
self._store.insert_reflection(record)  
  
def run_weekly_refinement(self) -> RefinementReport:  
"""æ¯å‘¨ä¸€æ¬¡æ‰¹é‡æç‚¼ï¼šhigh-confidence correct â†’ memory_patterns"""  
records = self._store.get_reflection_records(  
min_confidence=0.7, is_direction_correct=True)  
# æŒ‰ symbol + market_condition èšåˆï¼Œè®¡ç®—èƒœç‡  
groups = {}  
for r in records:  
key = f"{r.symbol}_{self._classify_market(r)}"  
groups.setdefault(key, []).append(r)  
added = updated = 0  
for key, group in groups.items():  
if len(group) < 10:  
continue  
win_rate = sum(1 for r in group if r.is_direction_correct) / len(group)  
if win_rate < 0.65:  
continue  
from pstds.memory.pattern import MemoryPattern  
from datetime import datetime  
pattern = MemoryPattern(  
pattern_key=key, description=f"Auto-refined: {key}",  
symbol=group[0].symbol, market_condition=self._classify_market(group[0]),  
win_rate=win_rate, sample_count=len(group),  
evidence_ids=[r.analysis_id for r in group[-10:]],  
last_updated=datetime.utcnow(), is_active=True,  
)  
existing = self._patterns.get_patterns(symbol=group[0].symbol)  
if any(p.pattern_key == key for p in existing):  
updated += 1  
else:  
added += 1  
self._patterns.add_or_update_pattern(pattern)  
return RefinementReport(patterns_added=added, patterns_updated=updated,  
samples_processed=len(records))  
  
def get_accuracy_trend(self, symbol: Optional[str] = None,  
months: int = 6) -> List[MonthlyAccuracy]:  
"""æœˆåº¦é¢„æµ‹å‡†ç¡®ç‡ï¼Œä¾› UI æŠ˜çº¿å›¾"""  
return self._store.get_monthly_accuracy(symbol=symbol, months=months)  
  
def _get_next_trading_day(self, d: date, symbol: str) -> date:  
return d + timedelta(days=1) # ç®€åŒ–ï¼šå®é™…éœ€æŸ¥ TradingCalendar  
  
def _fetch_actual_return(self, symbol: str, from_date: date, to_date: date):  
try:  
from pstds.temporal.context import TemporalContext  
ctx = TemporalContext.for_live(to_date)  
df = self._router.get_ohlcv(symbol, lookback_days=5, ctx=ctx)  
if df is not None and len(df) >= 2:  
return float((df.iloc[-1]['close'] - df.iloc[-2]['close']) / df.iloc[-2]['close'])  
except Exception:  
pass  
return None  
  
def _evaluate_direction(self, action: str, actual_return: Optional[float]) -> Optional[bool]:  
if actual_return is None:  
return None  
if action in ('BUY', 'STRONG_BUY'):  
return actual_return > 0  
if action in ('SELL', 'STRONG_SELL'):  
return actual_return < 0  
return None # HOLD ä¸è¯„ä¼°æ–¹å‘  
  
def _classify_market(self, record) -> str:  
return 'unknown' # å®é™…å®ç°éœ€åŸºäºå½“æ—¥ VIX/æ³¢åŠ¨ç‡
```

## 2.8 pstds/backtest/report.pyï¼ˆv3.0 æ–°å¢ï¼‰

```python
# pstds/backtest/report.py  
  
from dataclasses import dataclass, field  
from typing import List, Tuple  
from datetime import date  
import pandas as pd  
from pstds.backtest.performance import PerformanceMetrics  
  
@dataclass  
class DailyRecord:  
date: date  
action: str  
confidence: float  
debate_quality_score: float  
actual_return_next_day: float # å®é™…æ¬¡æ—¥æ¶¨è·Œå¹…  
pnl: float # å½“æ—¥ç›ˆäºï¼ˆç»å¯¹å€¼ï¼‰  
portfolio_value: float  
  
@dataclass  
class AttributionReport:  
signal_contribution: float # å¤šç©ºä¿¡å·è´¡çŒ®ï¼ˆ%ï¼‰  
volatility_adj_contribution: float # æ³¢åŠ¨ç‡è°ƒæ•´è´¡çŒ®ï¼ˆ%ï¼‰  
data_quality_impact: float # æ•°æ®è´¨é‡å½±å“ï¼ˆ%ï¼‰  
unexplained: float # æ®‹å·®  
  
@dataclass  
class BacktestReport:  
backtest_id: str  
symbol: str  
date_range: Tuple[date, date]  
config: dict  
performance: PerformanceMetrics  
nav_curve: pd.DataFrame # columns: [date, portfolio_value, benchmark_value]  
daily_records: List[DailyRecord]  
attribution: AttributionReport  
total_cost: dict # {tokens, usd}  
  
class BacktestReportGenerator:  
def generate(self, backtest_id: str, symbol: str,  
date_range: Tuple[date, date], config: dict,  
performance: PerformanceMetrics,  
daily_records: List[DailyRecord],  
benchmark_nav: pd.Series,  
total_cost: dict) -> BacktestReport:  
nav_curve = pd.DataFrame({  
'date': [r.date for r in daily_records],  
'portfolio_value': [r.portfolio_value for r in daily_records],  
'benchmark_value': benchmark_nav.values[:len(daily_records)],  
})  
attribution = self._calculate_attribution(daily_records)  
return BacktestReport(  
backtest_id=backtest_id, symbol=symbol, date_range=date_range,  
config=config, performance=performance, nav_curve=nav_curve,  
daily_records=daily_records, attribution=attribution, total_cost=total_cost,  
)  
  
def _calculate_attribution(self, records: List[DailyRecord]) -> AttributionReport:  
"""ç®€åŒ–å½’å› ï¼šæŒ‰ä¿¡å·æ­£ç¡®ç‡è®¡ç®—å„ç»´åº¦è´¡çŒ®"""  
if not records:  
return AttributionReport(0.0, 0.0, 0.0, 1.0)  
correct = [r for r in records if (r.action in ('BUY','STRONG_BUY') and r.actual_return_next_day > 0)  
or (r.action in ('SELL','STRONG_SELL') and r.actual_return_next_day < 0)]  
signal_contrib = len(correct) / len(records) * 0.7  
vol_contrib = sum(r.pnl for r in records if r.confidence > 0.8) / max(abs(sum(r.pnl for r in records)), 1e-6) * 0.15  
quality_impact = -0.05 if any(r.debate_quality_score < 5 for r in records) else 0.0  
unexplained = 1.0 - signal_contrib - abs(vol_contrib) - abs(quality_impact)  
return AttributionReport(  
signal_contribution=round(signal_contrib, 4),  
volatility_adj_contribution=round(vol_contrib, 4),  
data_quality_impact=round(quality_impact, 4),  
unexplained=round(max(unexplained, 0), 4),  
)  
  
def export_pdf(self, report: BacktestReport, output_path: str) -> None:  
from pstds.export.pdf_exporter import PDFExporter  
PDFExporter().export_backtest(report, output_path)  
  
def export_docx(self, report: BacktestReport, output_path: str) -> None:  
from pstds.export.docx_exporter import DocxExporter  
DocxExporter().export_backtest(report, output_path)  
  
def export_markdown(self, report: BacktestReport, output_path: str) -> None:  
from pstds.export.md_exporter import MarkdownExporter  
MarkdownExporter().export_backtest(report, output_path)
```

## 2.9 pstds/storage/models.pyï¼ˆv3.0 æ–°å¢ï¼‰

```python
# pstds/storage/models.py  
# MongoDB æ–‡æ¡£æ¨¡å‹å®šä¹‰ï¼ˆä½¿ç”¨ TypedDict æä¾›ç±»å‹æç¤ºï¼‰  
  
from typing import TypedDict, List, Optional  
from datetime import date, datetime  
  
class AnalysisDocument(TypedDict):  
_id: str # UUID  
symbol: str  
market_type: str # 'US'|'CN_A'|'HK'  
analysis_date: str # ISO date string  
created_at: datetime  
mode: str # 'LIVE'|'BACKTEST'  
config: dict # llm_provider, model, temperature, depth_level  
temporal: dict # compliant_news_count, filtered_news_count, violations  
data_quality: dict # score, missing_fields, anomaly_alerts, news_filter_stats  
reports: dict # market/sentiment/news/fundamentals/debate/trader/risk/final  
decision: dict # TradeDecision JSON  
input_hash: str # sha256 of (inputs + config)  
cost: dict # total_tokens, estimated_usd, actual_usd  
  
class PortfolioAnalysisDocument(TypedDict):  
_id: str # UUID = analysis_id  
symbols: List[str]  
analysis_date: str  
created_at: datetime  
correlation_matrix: dict # {symbol: {symbol: corr_value}}ï¼ˆDataFrame â†’ dictï¼‰  
high_correlation_pairs: List[dict] # [{sym1, sym2, corr}]  
sector_weights: dict  
sector_concentration_warning: bool  
portfolio_var_95: Optional[float]  
homogeneity_warning: bool  
data_quality_score: float  
position_advice: Optional[dict] # PositionAdvice JSONï¼ˆè‹¥æœ‰ï¼‰  
  
class ReflectionRecord(TypedDict):  
analysis_id: str  
symbol: str  
analysis_date: str  
predicted_action: str  
predicted_confidence: float  
actual_return_next_day: Optional[float]  
is_direction_correct: Optional[bool]  
created_at: str  
  
class MemoryPatternDocument(TypedDict):  
pattern_key: str # å”¯ä¸€é”®  
description: str  
symbol: Optional[str]  
market_condition: str  
win_rate: float  
sample_count: int  
evidence_ids: List[str]  
last_updated: datetime  
is_active: bool  
  
class BacktestResultDocument(TypedDict):  
_id: str  
symbol: str  
date_range: dict # {start, end}  
config: dict  
performance: dict # ç»©æ•ˆæŒ‡æ ‡  
daily_records: List[dict]  
attribution: dict  
nav_curve: List[dict] # [{date, portfolio_value, benchmark_value}]  
total_cost: dict  
created_at: datetime  
  
class CostRecord(TypedDict):  
analysis_id: str  
provider: str  
model: str  
input_tokens: int  
output_tokens: int  
cost_usd: float  
created_at: datetime
```

# 3. é…ç½®æ–‡ä»¶æ›´æ–°ï¼ˆdefault.yaml v3.0 å˜æ›´éƒ¨åˆ†ï¼‰

```python
# config/default.yaml â€” v3.0 æ–°å¢/å˜æ›´é…ç½®é¡¹ï¼ˆå…¶ä½™ä¸ v2.0 ä¸€è‡´ï¼‰  
  
# â”€â”€â”€ ç»„åˆåˆ†æé…ç½®ï¼ˆv3.0 æ–°å¢ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
portfolio:  
max_symbols: 20 # ç»„åˆåˆ†ææœ€å¤šè‚¡ç¥¨æ•°  
correlation_lookback_days: 60 # ç›¸å…³æ€§è®¡ç®—å›çœ‹å¤©æ•°  
high_corr_threshold: 0.70 # é«˜ç›¸å…³é¢„è­¦é˜ˆå€¼  
max_single_position: 0.30 # å•åªæœ€å¤§ä»“ä½  
max_corr_pair_position: 0.50 # é«˜ç›¸å…³å¯¹æ€»ä»“ä½ä¸Šé™  
sector_concentration_limit: 0.40 # è¡Œä¸šé›†ä¸­åº¦ä¸Šé™  
  
# â”€â”€â”€ è®°å¿†ç³»ç»Ÿé…ç½®ï¼ˆv3.0 è¡¥å…¨ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
memory:  
episodic_window_days: 90 # æƒ…æ™¯è®°å¿†æ»šåŠ¨çª—å£  
pattern_min_win_rate: 0.65 # æ¨¡å¼æç‚¼æœ€ä½èƒœç‡  
pattern_min_samples: 10 # æ¨¡å¼æç‚¼æœ€ä½æ ·æœ¬æ•°  
reflection_schedule: "0 2 * * MON" # åäº‹å®æç‚¼ Cronï¼ˆæ¯å‘¨ä¸€ 02:00ï¼‰  
reflection_confidence_threshold: 0.70 # çº³å…¥æç‚¼çš„æœ€ä½ç½®ä¿¡åº¦  
snapshot_max_count: 5 # çŸ­æœŸè®°å¿†å¿«ç…§æœ€å¤§ä¿ç•™æ•°  
embedding_provider: "local" # 'openai'|'local'ï¼ˆsentence-transformersï¼‰  
  
# â”€â”€â”€ æ–°é—»è¿‡æ»¤é…ç½®ï¼ˆv3.0 è¡¥å…¨ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
news_filter:  
relevance_threshold: 0.60 # ç¬¬äºŒçº§ç›¸å…³æ€§é˜ˆå€¼  
dedup_threshold: 0.85 # ç¬¬ä¸‰çº§å»é‡ç›¸ä¼¼åº¦é˜ˆå€¼  
  
# â”€â”€â”€ LLM é…ç½®ï¼ˆv3.0 æ–°å¢ DeepSeek/DashScopeï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
llm:  
# ... åŸæœ‰é…ç½®ä¸å˜ ...  
deepseek_base_url: "https://api.deepseek.com"  
dashscope_base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"  
# API Keys é€šè¿‡ç¯å¢ƒå˜é‡æ³¨å…¥ï¼Œç¦æ­¢å†™å…¥æ­¤æ–‡ä»¶  
# DEEPSEEK_API_KEY=...  
# DASHSCOPE_API_KEY=...
```

# 4. å¼€å‘è·¯çº¿å›¾ï¼ˆv3.0 æ›´æ–°ç‰ˆï¼‰

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>é˜¶æ®µ</strong></th>
<th><strong>ç›®æ ‡ç‰ˆæœ¬</strong></th>
<th><strong>ä¸»è¦äº¤ä»˜ç‰©</strong></th>
<th><strong>v3.0 çŠ¶æ€</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Phase 0-3<br />
(v0.1-v0.7)</td>
<td>v0.7</td>
<td>ç¯å¢ƒæ­å»ºã€æ—¶é—´éš”ç¦»ã€æ•°æ®å±‚ã€<br />
æ ¸å¿ƒ Web UIï¼ˆCCG v1.0 å®šä¹‰çš„é˜¶æ®µï¼‰</td>
<td>âœ… å·²å®Œæˆï¼ˆå« bug ä¿®å¤ï¼‰</td>
</tr>
<tr class="even">
<td>Phase 4<br />
(v0.9)</td>
<td>v0.9</td>
<td>å›æµ‹å¼•æ“ï¼ˆBacktestRunner ç­‰ï¼‰</td>
<td>âœ… å·²å®Œæˆï¼ˆexecutor/performance bug å·²ä¿®ï¼‰</td>
</tr>
<tr class="odd">
<td>Phase 5<br />
(v1.0)</td>
<td>v1.0</td>
<td>åŠŸèƒ½å®Œå–„ã€æ–‡æ¡£</td>
<td>âœ… å·²å®Œæˆï¼ˆéƒ¨åˆ†åŠŸèƒ½å¾…è¡¥å…¨ï¼‰</td>
</tr>
<tr class="even">
<td>Phase 6<br />
v3.0-A<br />
(2-3å‘¨)</td>
<td>v3.0-A</td>
<td>â‘  news_filter.pyï¼ˆä¸‰çº§è¿‡æ»¤ï¼‰<br />
â‘¡ backtest/report.py<br />
â‘¢ storage/models.py<br />
â‘£ llm/deepseek.py + dashscope.py<br />
â‘¤ å¯¹åº”å•å…ƒæµ‹è¯•ï¼ˆNF/BR ç³»åˆ—ï¼‰</td>
<td>ğŸ¯ ä¼˜å…ˆå®Œæˆ</td>
</tr>
<tr class="odd">
<td>Phase 7<br />
v3.0-B<br />
(2-3å‘¨)</td>
<td>v3.0-B</td>
<td>â‘  memory/ ä¸‰å±‚å®Œæ•´æ¶æ„<br />
â‘¡ scheduler å‘¨ä»»åŠ¡ï¼ˆReflectionEngineï¼‰<br />
â‘¢ UIï¼šhistory å‡†ç¡®ç‡è¶‹åŠ¿ã€backtest å½’å› <br />
â‘£ å¯¹åº”æµ‹è¯•ï¼ˆMEM/REF ç³»åˆ—ï¼‰</td>
<td>ğŸ¯ ç´§éšå…¶å</td>
</tr>
<tr class="even">
<td>Phase 8<br />
v3.0-C<br />
(3-4å‘¨)</td>
<td>v3.0-C</td>
<td>â‘  portfolio/ æ¨¡å—ï¼ˆPortfolioAnalyzer + PositionAdvisorï¼‰<br />
â‘¡ pages/08_portfolio_analysis.py<br />
â‘¢ ç»„åˆåˆ†æé›†æˆæµ‹è¯•ï¼ˆPA ç³»åˆ—ï¼‰<br />
â‘£ ç»„åˆæŠ¥å‘Šå¯¼å‡ºï¼ˆPDF/Wordï¼‰</td>
<td>ğŸ¯ æœ€å¤§æ–°åŠŸèƒ½</td>
</tr>
<tr class="odd">
<td>Phase 9<br />
v3.0-D<br />
(1-2å‘¨)</td>
<td>v3.0</td>
<td>â‘  Web UI å…¨é¢å‡çº§ï¼ˆå…¨å± K çº¿ã€æ·±è‰²ä¸»é¢˜ï¼‰<br />
â‘¡ æœ€ç»ˆéªŒè¯ï¼ˆæ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼‰<br />
â‘¢ docker-compose.yml æ›´æ–°<br />
â‘£ æ–‡æ¡£æ›´æ–°</td>
<td>ğŸ¯ æ”¶å°¾</td>
</tr>
<tr class="even">
<td>v3.xï¼ˆTBDï¼‰</td>
<td>v3.x</td>
<td>â‘  monkey-patch æ¶æ„é‡æ„ï¼ˆä¾èµ–æ³¨å…¥ï¼‰<br />
â‘¡ Trading-R1 æ­£å¼é›†æˆï¼ˆå¾…æ¨¡å‹å¼€æºï¼‰</td>
<td>â³ åç»­è§„åˆ’</td>
</tr>
</tbody>
</table>
